Documentación para entender el código

1) El pipeline correspondiente a la entrega final se encuentra dentro de la carpeta dag y se llama proyecto_final.py
Este proyecto tiene como objetivos
- "llamar" al codigo que esta dentro de la carpeta dag/entrega que se llama script.py
- se construye el DAG correspondiente a las tareas a ejecutarse en Airflow

2) En el script.py se lleva a cabo todo el proceso correspondiente a las entregas 1,2 y 3
Este sería el script principal el cual realiza un proceso ETL para: 
- Extraer datos climáticos de varias ciudades utilizando la API de HG Brasil
- Llevar a cabo las transformaciones correspondientes 
- Cargarlos en una tabla en Redshift.


3) La carpeta "Module o modulos es basicamente esa carpeta donde tenemos las funciones." 
En mi caso es la carpeta entrega que se encuentra dentro de la carpeta dags.
Dentro de esta carpeta se encuentra el script.py en el cual se tienen todas las funciones: 
a)def process_data():
    Función para disminuir el tiempo de espera e intentar solucionar el error
b)def conectar_redshift():
    Conexión a Redshift utilizando las credenciales definidas.
c)def crear_tabla_redshift(conn):
    Verificación de la existencia de la tabla en Redshift. Si no existe --> Creación.
d)def extraccion_datos(url):
    Solicitud GET a la URL proporcionada para extraer datos de la API pública.
    Si la solicitud es exitosa. Retorno de los datos en formato JSON
    Si la solicitud no es exitosa. Impresión de error
e)def transformacion_datos(data):
    Transformación de los datos extraídos de la API pública. 
    Eliminación de variables innecesarias. 
    Datos transformados en formato específico.
f) def combinar_datos(ciudades_por_pais, datos_transformados):
    Combinación de:
        Datos de la API
        Datos de las fuentes de datos
    Luego datos combinados para cada ciudad y país.
g) def carga_datos_redshift(conn, datos_carga):
    Carga de datos combinados en la tabla en Redshift
    Verificación de la existencia de registros previos
    No inserción de duplicados 
h) def etl_extraccion():
    Extracción de datos meteorológicos de múltiples localizaciones utilizando la API de HG Brasil.
    Retorna una lista de datos extraídos para cada WOEID en la lista `woeids`.
i) def etl_transformacion(lista_datos):
    Transformación de los datos extraídos de la API de HG Brasil.
    Retorna un diccionario de datos transformados, donde cada clave es el nombre de la ciudad.
j) def etl_combinar(ciudades_por_pais, datos_transformados):
    Combinación de datos meteorológicos transformados con información geográfica por ciudad y país.
    Retorna datos combinados estructurados para cada ciudad y país.
k) def etl_carga(datos_combinados):
    Carga de datos combinados en la tabla en Redshift después de conectar y crear la tabla si no existe.
    Retorna una lista de datos duplicados que no fueron insertados debido a su existencia previa.
l) def enviar_alerta_temperatura(ti):
m) def enviar_alerta_volumen(ti):
n) def enviar_email_alerta(asunto, cuerpo):
